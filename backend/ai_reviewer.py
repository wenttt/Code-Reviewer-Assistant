"""
AI Code Reviewer - AI Review Engine (Enhanced)
ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œä»£ç å®¡æŸ¥çš„æ ¸å¿ƒæ¨¡å— - å¢å¼ºç‰ˆ
æ”¯æŒï¼šæ•æ„Ÿä¿¡æ¯è¿‡æ»¤ã€å¤§PRåˆ†ç‰‡ã€ä¸Šä¸‹æ–‡å¢å¼ºã€æœ¬åœ°æ¨¡å‹
"""

import json
import os
import httpx
from typing import List, Optional, Dict, Any
from dataclasses import dataclass, asdict
from enum import Enum
from openai import AsyncOpenAI

try:
    import anthropic
    HAS_ANTHROPIC = True
except ImportError:
    HAS_ANTHROPIC = False

try:
    from google import genai
    from google.genai import types
    HAS_GOOGLE_GENAI = True
except ImportError:
    HAS_GOOGLE_GENAI = False

from github_client import PullRequest, FileChange
from security import SensitiveFilter, SecurityConfig, SensitiveMatch
from chunker import PRChunker, ReviewChunk, FileClassifier, FilePriority, aggregate_chunk_reviews, ChunkReviewResult
from context_analyzer import ContextEnhancer, FileContext, build_context_prompt, LanguageDetector


class IssueSeverity(str, Enum):
    CRITICAL = "critical"
    MAJOR = "major"
    MINOR = "minor"
    INFO = "info"


class IssueCategory(str, Enum):
    BUG = "bug"
    SECURITY = "security"
    PERFORMANCE = "performance"
    STYLE = "style"
    MAINTAINABILITY = "maintainability"
    BEST_PRACTICE = "best_practice"


class ModelProvider(str, Enum):
    OPENAI = "openai"
    DEEPSEEK = "deepseek"      # DeepSeek API
    ANTHROPIC = "anthropic"    # Anthropic Claude API
    GEMINI = "gemini"          # Google Gemini API
    AZURE = "azure"
    OLLAMA = "ollama"          # æœ¬åœ°æ¨¡å‹
    CUSTOM = "custom"          # è‡ªå®šä¹‰API (å…¬å¸è‡ªå»º/ç¬¬ä¸‰æ–¹å…¼å®¹OpenAIæ¥å£)


@dataclass
class ReviewIssue:
    severity: str
    category: str
    file: str
    line: Optional[int]
    description: str
    suggestion: Optional[str]


@dataclass
class ReviewResult:
    score: int
    summary: str
    issues: List[ReviewIssue]
    highlights: List[str]
    suggestions: List[str]
    # æ–°å¢å­—æ®µ
    filtered_secrets: List[Dict] = None  # è¢«è¿‡æ»¤çš„æ•æ„Ÿä¿¡æ¯
    chunks_count: int = 1  # åˆ†ç‰‡æ•°é‡
    skipped_files: List[str] = None  # è·³è¿‡çš„æ–‡ä»¶
    context_enhanced: bool = False  # æ˜¯å¦ä½¿ç”¨äº†ä¸Šä¸‹æ–‡å¢å¼º
    
    def to_dict(self):
        result = {
            "score": self.score,
            "summary": self.summary,
            "issues": [asdict(issue) for issue in self.issues],
            "highlights": self.highlights,
            "suggestions": self.suggestions,
            "chunks_count": self.chunks_count,
            "context_enhanced": self.context_enhanced,
        }
        if self.filtered_secrets:
            result["filtered_secrets_count"] = len(self.filtered_secrets)
            result["filtered_secrets"] = [
                {"type": s["type"], "file": s["file"], "line": s["line"]} 
                for s in (self.filtered_secrets or [])
            ]
        if self.skipped_files:
            result["skipped_files"] = self.skipped_files
        return result


@dataclass
class ReviewConfig:
    """å®¡æŸ¥é…ç½®"""
    # å®‰å…¨è®¾ç½®
    enable_security_filter: bool = True
    security_mode: str = "mask"  # mask | warn | block
    
    # åˆ†ç‰‡è®¾ç½®
    enable_chunking: bool = True
    max_tokens_per_chunk: int = 8000
    max_files_per_chunk: int = 15
    
    # ä¸Šä¸‹æ–‡è®¾ç½®
    enable_context_enhancement: bool = True
    fetch_full_files: bool = True
    
    # æ¨¡å‹è®¾ç½®
    provider: ModelProvider = ModelProvider.OPENAI
    model: str = "gpt-4o"
    temperature: float = 0.3
    
    # Ollamaè®¾ç½® (æœ¬åœ°æ¨¡å‹)
    ollama_base_url: str = "http://localhost:11434"
    
    # DeepSeekè®¾ç½®
    deepseek_base_url: str = "https://api.deepseek.com"
    
    # Anthropic Claudeè®¾ç½®
    anthropic_base_url: str = "https://api.anthropic.com"
    
    # Google Geminiè®¾ç½®
    gemini_base_url: str = ""  # ç•™ç©ºä½¿ç”¨é»˜è®¤
    
    # è‡ªå®šä¹‰APIè®¾ç½® (å…¼å®¹OpenAIæ¥å£çš„å…¬å¸å†…éƒ¨/ç¬¬ä¸‰æ–¹æœåŠ¡)
    custom_base_url: str = ""
    custom_model_name: str = ""


class AIReviewEngine:
    """AIä»£ç å®¡æŸ¥å¼•æ“ - å¢å¼ºç‰ˆ"""
    
    SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ä»£ç å®¡æŸ¥ä¸“å®¶ï¼Œæ‹¥æœ‰10å¹´ä»¥ä¸Šçš„è½¯ä»¶å¼€å‘ç»éªŒã€‚ä½ ç²¾é€šå¤šç§ç¼–ç¨‹è¯­è¨€ï¼Œå¯¹ä»£ç è´¨é‡ã€å®‰å…¨æ€§ã€æ€§èƒ½ä¼˜åŒ–æœ‰æ·±å…¥ç†è§£ã€‚

ä½ çš„ä»»åŠ¡æ˜¯å¯¹Pull Requestçš„ä»£ç å˜æ›´è¿›è¡Œå…¨é¢ã€ä¸“ä¸šçš„å®¡æŸ¥ã€‚

## å®¡æŸ¥ç»´åº¦

1. **ä»£ç è´¨é‡** (æƒé‡30%)
   - ä»£ç å¯è¯»æ€§å’Œæ¸…æ™°åº¦
   - å‘½åè§„èŒƒï¼ˆå˜é‡ã€å‡½æ•°ã€ç±»ï¼‰
   - ä»£ç ç»“æ„å’Œç»„ç»‡
   - æ³¨é‡Šçš„å……åˆ†æ€§å’Œå‡†ç¡®æ€§

2. **æ½œåœ¨Bug** (æƒé‡25%)
   - é€»è¾‘é”™è¯¯
   - è¾¹ç•Œæ¡ä»¶å¤„ç†
   - ç©ºå€¼/å¼‚å¸¸å¤„ç†
   - ç±»å‹å®‰å…¨

3. **å®‰å…¨é—®é¢˜** (æƒé‡20%)
   - è¾“å…¥éªŒè¯
   - SQLæ³¨å…¥ã€XSSç­‰æ³¨å…¥æ”»å‡»
   - æ•æ„Ÿä¿¡æ¯æ³„éœ²
   - è®¤è¯å’Œæˆæƒé—®é¢˜

4. **æ€§èƒ½é—®é¢˜** (æƒé‡15%)
   - ç®—æ³•æ•ˆç‡
   - èµ„æºä½¿ç”¨ï¼ˆå†…å­˜ã€CPUï¼‰
   - æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–
   - ç¼“å­˜ä½¿ç”¨

5. **æœ€ä½³å®è·µ** (æƒé‡10%)
   - è®¾è®¡æ¨¡å¼åº”ç”¨
   - ä»£ç å¤ç”¨
   - æµ‹è¯•è¦†ç›–
   - æ–‡æ¡£å®Œæ•´æ€§

## ä¸Šä¸‹æ–‡åˆ†æ

å½“æä¾›äº†å®Œæ•´æ–‡ä»¶å†…å®¹æ—¶ï¼Œè¯·ç‰¹åˆ«æ³¨æ„ï¼š
- ä¿®æ”¹çš„å‡½æ•°æ˜¯å¦å½±å“äº†å…¶ä»–è°ƒç”¨å®ƒçš„åœ°æ–¹
- æ–°å¢çš„ä»£ç æ˜¯å¦ä¸ç°æœ‰ä»£ç é£æ ¼ä¸€è‡´
- å¯¼å…¥çš„æ¨¡å—æ˜¯å¦è¢«æ­£ç¡®ä½¿ç”¨
- æ˜¯å¦å­˜åœ¨æœªä½¿ç”¨çš„å¯¼å…¥æˆ–å˜é‡

## è¾“å‡ºæ ¼å¼

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºå®¡æŸ¥ç»“æœï¼š

```json
{
  "score": 85,
  "summary": "æ€»ä½“è¯„ä»·ï¼Œ100å­—ä»¥å†…",
  "issues": [
    {
      "severity": "critical|major|minor|info",
      "category": "bug|security|performance|style|maintainability|best_practice",
      "file": "æ–‡ä»¶è·¯å¾„",
      "line": è¡Œå·æˆ–null,
      "description": "é—®é¢˜æè¿°",
      "suggestion": "ä¿®æ”¹å»ºè®®"
    }
  ],
  "highlights": ["ä»£ç äº®ç‚¹1", "ä»£ç äº®ç‚¹2"],
  "suggestions": ["æ”¹è¿›å»ºè®®1", "æ”¹è¿›å»ºè®®2"]
}
```

## è¯„åˆ†æ ‡å‡†

- 90-100: ä¼˜ç§€ï¼Œä»£ç è´¨é‡é«˜ï¼Œå‡ ä¹æ— é—®é¢˜
- 80-89: è‰¯å¥½ï¼Œæœ‰å°‘é‡å°é—®é¢˜
- 70-79: ä¸€èˆ¬ï¼Œæœ‰ä¸€äº›éœ€è¦æ”¹è¿›çš„åœ°æ–¹
- 60-69: è¾ƒå·®ï¼Œå­˜åœ¨æ˜æ˜¾é—®é¢˜
- 0-59: éœ€è¦é‡å¤§ä¿®æ”¹

## æ³¨æ„äº‹é¡¹

1. å®¡æŸ¥è¦å®¢è§‚ã€ä¸“ä¸šï¼Œç»™å‡ºå…·ä½“çš„é—®é¢˜æè¿°å’Œæ”¹è¿›å»ºè®®
2. åŒºåˆ†é—®é¢˜çš„ä¸¥é‡ç¨‹åº¦ï¼Œä¸è¦æŠŠæ‰€æœ‰é—®é¢˜éƒ½æ ‡ä¸ºcritical
3. ä¸ä»…æŒ‡å‡ºé—®é¢˜ï¼Œä¹Ÿè¦è‚¯å®šä»£ç ä¸­çš„ä¼˜ç‚¹
4. å»ºè®®è¦å…·ä½“å¯è¡Œï¼Œæœ€å¥½èƒ½ç»™å‡ºä»£ç ç¤ºä¾‹
5. è€ƒè™‘ä»£ç çš„ä¸Šä¸‹æ–‡å’Œé¡¹ç›®ç‰¹ç‚¹
6. å¦‚æœä»£ç ä¸­æœ‰ [REDACTED] æˆ– *** æ ‡è®°ï¼Œè¯´æ˜æ•æ„Ÿä¿¡æ¯å·²è¢«è¿‡æ»¤ï¼Œä¸è¦å¯¹æ­¤æå‡ºé—®é¢˜"""

    def __init__(
        self, 
        api_key: str, 
        base_url: Optional[str] = None,
        config: Optional[ReviewConfig] = None
    ):
        self.config = config or ReviewConfig()
        self.api_key = api_key
        self.security_filter = SensitiveFilter()
        self.chunker = PRChunker(
            max_tokens=self.config.max_tokens_per_chunk,
            max_files=self.config.max_files_per_chunk
        )
        
        # åˆå§‹åŒ–AIå®¢æˆ·ç«¯ (æ ¹æ®provideré€‰æ‹©ä¸åŒçš„å®¢æˆ·ç«¯)
        self.client = None           # OpenAIå…¼å®¹å®¢æˆ·ç«¯
        self.anthropic_client = None # AnthropicåŸç”Ÿå®¢æˆ·ç«¯
        self.gemini_client = None    # Google Geminiå®¢æˆ·ç«¯
        
        if self.config.provider == ModelProvider.OLLAMA:
            # Ollamaä½¿ç”¨OpenAIå…¼å®¹æ¥å£
            self.client = AsyncOpenAI(
                api_key="ollama",
                base_url=f"{self.config.ollama_base_url}/v1"
            )
        elif self.config.provider == ModelProvider.DEEPSEEK:
            # DeepSeekä½¿ç”¨OpenAIå…¼å®¹æ¥å£
            self.client = AsyncOpenAI(
                api_key=api_key,
                base_url=base_url or self.config.deepseek_base_url
            )
        elif self.config.provider == ModelProvider.ANTHROPIC:
            # Anthropic Claude ä½¿ç”¨åŸç”ŸSDK
            if not HAS_ANTHROPIC:
                raise ImportError(
                    "è¯·å®‰è£… anthropic åŒ…: pip install anthropic"
                )
            client_kwargs = {"api_key": api_key}
            if base_url:
                client_kwargs["base_url"] = base_url
            elif self.config.anthropic_base_url and self.config.anthropic_base_url != "https://api.anthropic.com":
                client_kwargs["base_url"] = self.config.anthropic_base_url
            self.anthropic_client = anthropic.AsyncAnthropic(**client_kwargs)
        elif self.config.provider == ModelProvider.GEMINI:
            # Google Gemini ä½¿ç”¨åŸç”ŸSDK
            if not HAS_GOOGLE_GENAI:
                raise ImportError(
                    "è¯·å®‰è£… google-genai åŒ…: pip install google-genai"
                )
            self.gemini_client = genai.Client(api_key=api_key)
        elif self.config.provider == ModelProvider.CUSTOM:
            # è‡ªå®šä¹‰API - å…¼å®¹OpenAIæ¥å£ (å…¬å¸è‡ªå»º/ç¬¬ä¸‰æ–¹æœåŠ¡)
            custom_url = base_url or self.config.custom_base_url
            if not custom_url:
                raise ValueError(
                    "è‡ªå®šä¹‰APIéœ€è¦æä¾› Base URLï¼Œè¯·å¡«å†™æ‚¨çš„APIæœåŠ¡åœ°å€ï¼Œ"
                    "ä¾‹å¦‚: https://your-company.com/v1"
                )
            self.client = AsyncOpenAI(
                api_key=api_key or "custom-key",
                base_url=custom_url
            )
        else:
            # OpenAI å®˜æ–¹
            client_kwargs = {"api_key": api_key}
            if base_url:
                client_kwargs["base_url"] = base_url
            self.client = AsyncOpenAI(**client_kwargs)
    
    def _filter_sensitive_content(
        self, 
        files: List[FileChange]
    ) -> tuple[List[FileChange], List[SensitiveMatch]]:
        """è¿‡æ»¤æ–‡ä»¶ä¸­çš„æ•æ„Ÿä¿¡æ¯"""
        if not self.config.enable_security_filter:
            return files, []
        
        filtered_files = []
        all_matches = []
        
        for file in files:
            if file.patch:
                filtered_patch, matches = self.security_filter.filter_content(
                    file.patch, 
                    file.filename
                )
                # åˆ›å»ºæ–°çš„FileChangeå¯¹è±¡ï¼Œé¿å…ä¿®æ”¹åŸå¯¹è±¡
                filtered_file = FileChange(
                    filename=file.filename,
                    status=file.status,
                    additions=file.additions,
                    deletions=file.deletions,
                    changes=file.changes,
                    patch=filtered_patch
                )
                filtered_files.append(filtered_file)
                all_matches.extend(matches)
            else:
                filtered_files.append(file)
        
        return filtered_files, all_matches
    
    def _prepare_review_content(
        self, 
        pr: PullRequest, 
        files: List[FileChange],
        context: Optional[List[FileContext]] = None
    ) -> str:
        """å‡†å¤‡å®¡æŸ¥å†…å®¹"""
        content_parts = []
        
        # PRåŸºæœ¬ä¿¡æ¯
        content_parts.append("## Pull Request ä¿¡æ¯")
        content_parts.append(f"- **æ ‡é¢˜**: {pr.title}")
        content_parts.append(f"- **æè¿°**: {pr.body or 'æ— æè¿°'}")
        content_parts.append(f"- **åˆ†æ”¯**: {pr.head_ref} â†’ {pr.base_ref}")
        content_parts.append(f"- **å˜æ›´ç»Ÿè®¡**: +{pr.additions} -{pr.deletions} è¡Œï¼Œ{pr.changed_files} ä¸ªæ–‡ä»¶")
        content_parts.append("")
        
        # å¦‚æœæœ‰å®Œæ•´ä¸Šä¸‹æ–‡
        if context:
            content_parts.append("## å®Œæ•´æ–‡ä»¶å†…å®¹ï¼ˆç”¨äºä¸Šä¸‹æ–‡ç†è§£ï¼‰")
            for ctx in context:
                content_parts.append(f"\n### {ctx.filename}")
                if ctx.functions:
                    func_names = [f.name for f in ctx.functions[:10]]
                    content_parts.append(f"**å®šä¹‰çš„å‡½æ•°**: {', '.join(func_names)}")
                # é™åˆ¶å†…å®¹é•¿åº¦
                file_content = ctx.content
                if len(file_content) > 3000:
                    file_content = file_content[:3000] + "\n... (å†…å®¹æˆªæ–­)"
                content_parts.append(f"```{ctx.language.value}")
                content_parts.append(file_content)
                content_parts.append("```")
            content_parts.append("")
        
        # æ–‡ä»¶å˜æ›´è¯¦æƒ…
        content_parts.append("## å˜æ›´æ–‡ä»¶åˆ—è¡¨")
        for f in files:
            priority = FileClassifier.get_priority(f)
            priority_emoji = {
                FilePriority.CRITICAL: "ğŸ”´",
                FilePriority.HIGH: "ğŸŸ ", 
                FilePriority.MEDIUM: "ğŸŸ¡",
                FilePriority.LOW: "ğŸŸ¢",
            }.get(priority, "âšª")
            status_emoji = {"added": "ğŸ†•", "modified": "ğŸ“", "deleted": "ğŸ—‘ï¸", "renamed": "ğŸ“‹"}.get(f.status, "ğŸ“„")
            content_parts.append(f"- {priority_emoji}{status_emoji} `{f.filename}` (+{f.additions} -{f.deletions})")
        content_parts.append("")
        
        # ä»£ç Diff
        content_parts.append("## ä»£ç å˜æ›´ (Diff)")
        for f in files:
            if f.patch:
                content_parts.append(f"\n### {f.filename}")
                content_parts.append("```diff")
                patch = f.patch
                if len(patch) > 4000:
                    patch = patch[:4000] + "\n... (å†…å®¹æˆªæ–­)"
                content_parts.append(patch)
                content_parts.append("```")
        
        return "\n".join(content_parts)
    
    def _parse_review_response(self, response_text: str) -> Dict:
        """è§£æAIå“åº”"""
        try:
            text = response_text.strip()
            if text.startswith("```json"):
                text = text[7:]
            if text.startswith("```"):
                text = text[3:]
            if text.endswith("```"):
                text = text[:-3]
            text = text.strip()
            
            return json.loads(text)
        except json.JSONDecodeError as e:
            return {
                "score": 50,
                "summary": f"å®¡æŸ¥ç»“æœè§£æå¤±è´¥: {str(e)}",
                "issues": [],
                "highlights": [],
                "suggestions": ["AIè¿”å›çš„ç»“æœæ ¼å¼å¼‚å¸¸ï¼Œè¯·é‡è¯•"]
            }
    
    async def _call_model(self, content: str) -> str:
        """è°ƒç”¨AIæ¨¡å‹ - æ ¹æ®providerè·¯ç”±åˆ°ä¸åŒçš„API"""
        user_message = f"è¯·å®¡æŸ¥ä»¥ä¸‹Pull Request:\n\n{content}"
        
        if self.config.provider == ModelProvider.ANTHROPIC:
            return await self._call_anthropic(user_message)
        elif self.config.provider == ModelProvider.GEMINI:
            return await self._call_gemini(user_message)
        else:
            # OpenAI / DeepSeek / Ollama / Custom éƒ½èµ°OpenAIå…¼å®¹æ¥å£
            return await self._call_openai_compatible(user_message)
    
    async def _call_openai_compatible(self, user_message: str) -> str:
        """è°ƒç”¨OpenAIå…¼å®¹æ¥å£ (OpenAI/DeepSeek/Ollama/Custom)"""
        response = await self.client.chat.completions.create(
            model=self.config.model,
            messages=[
                {"role": "system", "content": self.SYSTEM_PROMPT},
                {"role": "user", "content": user_message}
            ],
            temperature=self.config.temperature,
            max_tokens=4000
        )
        return response.choices[0].message.content
    
    async def _call_anthropic(self, user_message: str) -> str:
        """è°ƒç”¨Anthropic Claude API"""
        response = await self.anthropic_client.messages.create(
            model=self.config.model,
            max_tokens=4000,
            temperature=self.config.temperature,
            system=self.SYSTEM_PROMPT,
            messages=[
                {"role": "user", "content": user_message}
            ]
        )
        # Claudeè¿”å›çš„contentæ˜¯ä¸€ä¸ªåˆ—è¡¨
        return response.content[0].text
    
    async def _call_gemini(self, user_message: str) -> str:
        """è°ƒç”¨Google Gemini API"""
        full_prompt = f"{self.SYSTEM_PROMPT}\n\n{user_message}"
        response = await self.gemini_client.aio.models.generate_content(
            model=self.config.model,
            contents=full_prompt,
            config=types.GenerateContentConfig(
                temperature=self.config.temperature,
                max_output_tokens=4000,
            ),
        )
        return response.text
    
    async def review(
        self, 
        pr: PullRequest,
        github_token: Optional[str] = None,  # ç”¨äºè·å–å®Œæ•´æ–‡ä»¶å†…å®¹
        owner: Optional[str] = None,
        repo: Optional[str] = None
    ) -> ReviewResult:
        """æ‰§è¡Œä»£ç å®¡æŸ¥ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        
        if not pr.files:
            return ReviewResult(
                score=100,
                summary="æ²¡æœ‰ä»£ç å˜æ›´éœ€è¦å®¡æŸ¥",
                issues=[],
                highlights=[],
                suggestions=[]
            )
        
        # Step 1: å®‰å…¨è¿‡æ»¤
        filtered_files, sensitive_matches = self._filter_sensitive_content(pr.files)
        
        # è®°å½•è¢«è¿‡æ»¤çš„æ•æ„Ÿä¿¡æ¯
        filtered_secrets = [
            {"type": m.type.value, "file": m.file, "line": m.line_number}
            for m in sensitive_matches
        ]
        
        # Step 2: åˆ†ç‰‡å¤„ç†
        if self.config.enable_chunking and len(filtered_files) > self.config.max_files_per_chunk:
            chunks = self.chunker.create_chunks(filtered_files)
        else:
            chunks = [ReviewChunk(
                chunk_id=1,
                files=filtered_files,
                total_lines=sum(f.additions + f.deletions for f in filtered_files),
                estimated_tokens=0,
                priority=FilePriority.HIGH
            )]
        
        # è·å–è·³è¿‡çš„æ–‡ä»¶åˆ—è¡¨
        skipped_files = [
            f.filename for f in pr.files 
            if FileClassifier.get_priority(f) == FilePriority.SKIP
        ]
        
        # Step 3: ä¸Šä¸‹æ–‡å¢å¼ºï¼ˆå¯é€‰ï¼‰
        context = None
        if (self.config.enable_context_enhancement and 
            github_token and owner and repo):
            try:
                enhancer = ContextEnhancer(github_token)
                file_dicts = [{"filename": f.filename, "status": f.status} for f in filtered_files]
                context = await enhancer.get_full_context(owner, repo, file_dicts, pr.head_ref)
            except Exception as e:
                # ä¸Šä¸‹æ–‡è·å–å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
                print(f"Warning: Failed to get context: {e}")
        
        # Step 4: æ‰§è¡Œå®¡æŸ¥
        try:
            if len(chunks) == 1:
                # å•åˆ†ç‰‡ç›´æ¥å®¡æŸ¥
                review_content = self._prepare_review_content(pr, chunks[0].files, context)
                response_text = await self._call_model(review_content)
                result_data = self._parse_review_response(response_text)
                
                issues = [
                    ReviewIssue(
                        severity=i.get("severity", "info"),
                        category=i.get("category", "style"),
                        file=i.get("file", "unknown"),
                        line=i.get("line"),
                        description=i.get("description", ""),
                        suggestion=i.get("suggestion")
                    )
                    for i in result_data.get("issues", [])
                ]
                
                return ReviewResult(
                    score=result_data.get("score", 70),
                    summary=result_data.get("summary", "å®¡æŸ¥å®Œæˆ"),
                    issues=issues,
                    highlights=result_data.get("highlights", []),
                    suggestions=result_data.get("suggestions", []),
                    filtered_secrets=filtered_secrets,
                    chunks_count=1,
                    skipped_files=skipped_files,
                    context_enhanced=context is not None
                )
            else:
                # å¤šåˆ†ç‰‡é€ä¸ªå®¡æŸ¥
                chunk_results = []
                for chunk in chunks:
                    review_content = self._prepare_review_content(pr, chunk.files, None)
                    response_text = await self._call_model(review_content)
                    result_data = self._parse_review_response(response_text)
                    
                    chunk_results.append(ChunkReviewResult(
                        chunk_id=chunk.chunk_id,
                        score=result_data.get("score", 70),
                        issues=result_data.get("issues", []),
                        summary=result_data.get("summary", "")
                    ))
                
                # èšåˆç»“æœ
                aggregated = aggregate_chunk_reviews(
                    chunk_results, 
                    skipped_files, 
                    len(pr.files)
                )
                
                # åˆå¹¶æ‰€æœ‰issues
                all_issues = []
                for cr in chunk_results:
                    for i in cr.issues:
                        all_issues.append(ReviewIssue(
                            severity=i.get("severity", "info"),
                            category=i.get("category", "style"),
                            file=i.get("file", "unknown"),
                            line=i.get("line"),
                            description=i.get("description", ""),
                            suggestion=i.get("suggestion")
                        ))
                
                return ReviewResult(
                    score=aggregated.overall_score,
                    summary=f"{aggregated.summary}ã€‚å…±å‘ç°{aggregated.total_issues}ä¸ªé—®é¢˜ã€‚",
                    issues=all_issues,
                    highlights=[],
                    suggestions=[],
                    filtered_secrets=filtered_secrets,
                    chunks_count=len(chunks),
                    skipped_files=skipped_files,
                    context_enhanced=False
                )
                
        except Exception as e:
            return ReviewResult(
                score=0,
                summary=f"å®¡æŸ¥è¿‡ç¨‹å‡ºé”™: {str(e)}",
                issues=[],
                highlights=[],
                suggestions=["è¯·æ£€æŸ¥APIé…ç½®åé‡è¯•"],
                filtered_secrets=filtered_secrets,
                skipped_files=skipped_files
            )
    
    # ä¿ç•™æ—§æ¥å£çš„å…¼å®¹æ€§
    async def review_simple(self, pr: PullRequest, model: str = "gpt-4o") -> ReviewResult:
        """ç®€å•å®¡æŸ¥æ¨¡å¼ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰"""
        self.config.model = model
        self.config.enable_context_enhancement = False
        return await self.review(pr)
